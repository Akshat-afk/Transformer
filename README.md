# Transformer

A from-scratch implementation of the Transformer architecture, built for educational and self-learning purposes. This repository walks through every component of the Transformer model, originally proposed in the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762).

## 🚀 Purpose

This project aims to:

- Deepen understanding of self-attention, positional encoding, and multi-head attention
- Explore how sequence modeling works without recurrence
- Serve as a learning reference for others studying transformers or deep learning

## 🧠 Key Features

- Encoder and Decoder blocks implemented step-by-step
- Scaled Dot-Product Attention
- Positional Encoding (sinusoidal)
- Cross-attention between encoder and decoder
- Full forward pass for training/inference
- Minimal dependencies, clean and readable code


## 🛠️ Dependencies

- Python 3.8+
- PyTorch >= 1.10
- NumPy


## 📝 Status

✅ Core architecture  
✅ Training loop / tokenizer  
✅ Dataset integration  

## Presentation

[*Presentation*](https://www.canva.com/design/DAGrWXPRzCk/OXrrrV4LCdotJTFtiFjTyQ/view?utm_content=DAGrWXPRzCk&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h3589fcd8af#6
) on the theoretical aspect

## 🙌 Acknowledgements

Inspired by:

- Vaswani et al., 2017 — *Attention Is All You Need*
- The Annotated Transformer by Harvard NLP
- TensorFlow/NLP & PyTorch tutorials

## 📜 License

MIT License
