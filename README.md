# Transformer

A from-scratch implementation of the Transformer architecture, built for educational and self-learning purposes. This repository walks through every component of the Transformer model, originally proposed in the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762).

## ğŸš€ Purpose

This project aims to:

- Deepen understanding of self-attention, positional encoding, and multi-head attention
- Explore how sequence modeling works without recurrence
- Serve as a learning reference for others studying transformers or deep learning

## ğŸ§  Key Features

- Encoder and Decoder blocks implemented step-by-step
- Scaled Dot-Product Attention
- Positional Encoding (sinusoidal)
- Cross-attention between encoder and decoder
- Full forward pass for training/inference
- Minimal dependencies, clean and readable code


## ğŸ› ï¸ Dependencies

- Python 3.8+
- PyTorch >= 1.10
- NumPy


## ğŸ“ Status

âœ… Core architecture  
âœ… Training loop / tokenizer  
âœ… Dataset integration  

## Presentation

[*Presentation*](https://www.canva.com/design/DAGrWXPRzCk/OXrrrV4LCdotJTFtiFjTyQ/view?utm_content=DAGrWXPRzCk&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h3589fcd8af#6
) on the theoretical aspect

## ğŸ™Œ Acknowledgements

Inspired by:

- Vaswani et al., 2017 â€” *Attention Is All You Need*
- The Annotated Transformer by Harvard NLP
- TensorFlow/NLP & PyTorch tutorials

## ğŸ“œ License

MIT License
