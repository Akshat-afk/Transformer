# Transformer

A from-scratch implementation of the Transformer architecture, built for educational and self-learning purposes. This repository walks through every component of the Transformer model, originally proposed in the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762).

## ğŸš€ Purpose

This project aims to:

- Deepen understanding of self-attention, positional encoding, and multi-head attention
- Explore how sequence modeling works without recurrence
- Serve as a learning reference for others studying transformers or deep learning

## ğŸ§  Key Features

- Encoder and Decoder blocks implemented step-by-step
- Scaled Dot-Product Attention and Multi-Head Attention
- Positional Encoding (sinusoidal)
- Masking for autoregressive decoding
- Cross-attention between encoder and decoder
- Full forward pass for training/inference
- Minimal dependencies, clean and readable code

## ğŸ“ Structure

```
transformer/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ attention.py         # Scaled dot-product & multi-head attention
â”‚   â”œâ”€â”€ encoder.py           # Encoder block
â”‚   â”œâ”€â”€ decoder.py           # Decoder block
â”‚   â”œâ”€â”€ transformer.py       # Full Transformer architecture
â”‚   â””â”€â”€ positional_encoding.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ masks.py             # Padding and look-ahead masks
â”œâ”€â”€ train.py                 # Example training loop (optional)
â”œâ”€â”€ vocab.py                 # Simple tokenizer/vocab utilities
â”œâ”€â”€ config.py                # Hyperparameter config
â””â”€â”€ README.md
```

## ğŸ› ï¸ Dependencies

- Python 3.8+
- PyTorch >= 1.10
- NumPy

Install using:

```bash
pip install -r requirements.txt
```

## ğŸ§ª Example Usage

```python
from model.transformer import Transformer

model = Transformer(
    num_layers=6,
    d_model=512,
    num_heads=8,
    dff=2048,
    input_vocab_size=8000,
    target_vocab_size=8000,
    max_pos_encoding=10000
)
```

## ğŸ“ Status

âœ… Core architecture  
ğŸŸ¡ Training loop / tokenizer  
ğŸ”² Dataset integration  

## ğŸ§­ Roadmap

- [ ] Add training script on toy data (e.g., Copy task, Translation)
- [ ] Integrate positional encoding visualizations
- [ ] Add support for BPE/tokenizers

## ğŸ™Œ Acknowledgements

Inspired by:

- Vaswani et al., 2017 â€” *Attention Is All You Need*
- The Annotated Transformer by Harvard NLP
- TensorFlow/NLP & PyTorch tutorials

## ğŸ“œ License

MIT License
