{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading dataset...\n",
      "🔡 Vocab size (characters): 56\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "# ================== Load and Preprocess Text ==================\n",
    "print(\"📥 Loading dataset...\")\n",
    "with open(\"shakespeare_full_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read().lower()\n",
    "\n",
    "# ================== Tokenization (Character-Level) ==================\n",
    "chars = list(text_data)\n",
    "vocab = sorted(set(chars))\n",
    "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "print(f\"🔡 Vocab size (characters): {vocab_size}\")\n",
    "\n",
    "# ================== Sequence Preparation ==================\n",
    "sequence_len = 128\n",
    "inputs, targets = [], []\n",
    "for i in range(len(chars) - sequence_len):\n",
    "    inputs.append([char2idx[c] for c in chars[i:i+sequence_len]])\n",
    "    targets.append([char2idx[c] for c in chars[i+1:i+sequence_len+1]])\n",
    "\n",
    "input_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "target_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# ================== DataLoader ==================\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(input_tensor, target_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# ================== Transformer Model ==================\n",
    "from Transformer import Transformer\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "model = Transformer(\n",
    "    vocabulary_size=vocab_size,\n",
    "    number_of_embeddings=embedding_dim,\n",
    "    sequence_len=sequence_len,\n",
    "    input_dimensions=embedding_dim,\n",
    ").to(device)\n",
    "\n",
    "# summary(model, input_size=(1, sequence_len), dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e1a1b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 78450/78450 [31:30<00:00, 41.51it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3066.7395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 78450/78450 [25:09<00:00, 51.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1651.1622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 78450/78450 [23:16<00:00, 56.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1574.0356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 78450/78450 [26:23<00:00, 49.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1516.0804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 78450/78450 [25:58<00:00, 50.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1474.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 78450/78450 [27:41<00:00, 47.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 1460.5759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 78450/78450 [27:48<00:00, 47.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 1447.3545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 78450/78450 [28:25<00:00, 45.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 1426.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 78450/78450 [28:11<00:00, 46.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 1377.8276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 78450/78450 [31:44<00:00, 41.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1342.0834\n",
      "✅ Model saved to 'word_level_transformer.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================== Training Setup ==================\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ================== Training Loop ==================\n",
    "print(\"Training started...\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# ================== Save Model ==================\n",
    "torch.save(model.state_dict(), \"word_level_transformer.pth\")\n",
    "print(\"✅ Model saved to 'word_level_transformer.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf473f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and ready\n",
      "🧠 Seed: 'King:\\n'\n",
      "\n",
      "📝 Generated Text:\n",
      "\n",
      "                                                                                                                          king:\n",
      "this,\n",
      "win hou she men.\n",
      "hee.\n",
      "hin hount.\n",
      "the hon ho sto shens whon thon whin here whou ther'stron ther.\n",
      "[houstron rone the son heard sanger wan do the mane the sore on what be what.\n",
      "\n",
      "he maren anter has le the the in i she lere the whath les mangin the sen man.\n",
      "\n",
      "prestres the deas hee mereting and and the houstre.\n",
      "\n",
      "and fortherer warle the has she there the be hale with the the that what of me isherated.\n",
      "\n",
      "his he sheresed i wentere her for the he mee the the the she hons exit the lethe heer and sene m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from Transformer import Transformer  # Your trained model\n",
    "\n",
    "# ====== Load and Prepare Vocab ======\n",
    "with open(\"shakespeare_full_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read().lower()\n",
    "\n",
    "chars = list(text_data)\n",
    "vocab = sorted(set(chars))\n",
    "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# ====== Model Setup ======\n",
    "embedding_dim = 128\n",
    "sequence_len = 128\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Transformer(\n",
    "    vocabulary_size=vocab_size,\n",
    "    number_of_embeddings=embedding_dim,\n",
    "    sequence_len=sequence_len,\n",
    "    input_dimensions=embedding_dim,\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"word_level_transformer.pth\", map_location=device))\n",
    "model.eval()\n",
    "print(\"✅ Model loaded and ready\")\n",
    "\n",
    "# ====== Seed Input ======\n",
    "seed_text = input(\"Enter seed text (default: 'King:\\\\n'): \").strip()\n",
    "if not seed_text:\n",
    "    seed_text = \"King:\\n\"\n",
    "\n",
    "print(\"🧠 Seed:\", repr(seed_text))\n",
    "\n",
    "seed_chars = list(seed_text.lower())\n",
    "seed_ids = [char2idx.get(c, char2idx[' ']) for c in seed_chars]\n",
    "\n",
    "# Pad/trim seed to sequence length\n",
    "if len(seed_ids) < sequence_len:\n",
    "    seed_ids = [char2idx[' ']] * (sequence_len - len(seed_ids)) + seed_ids\n",
    "else:\n",
    "    seed_ids = seed_ids[-sequence_len:]\n",
    "\n",
    "generated = seed_ids.copy()\n",
    "\n",
    "# ====== Generation Settings ======\n",
    "num_generate = 500  # total characters to generate\n",
    "temperature = 0.7\n",
    "top_p = 0.85\n",
    "\n",
    "# ====== Generation Loop ======\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_generate):\n",
    "        x = torch.tensor([generated[-sequence_len:]], dtype=torch.long).to(device)\n",
    "        logits = model(x)[0, -1] / temperature\n",
    "\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        probs = F.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "\n",
    "        # Top-p filtering\n",
    "        cutoff = cumulative_probs > top_p\n",
    "        cutoff[0] = False  # keep top token\n",
    "        sorted_logits[cutoff] = float('-inf')\n",
    "        filtered_probs = F.softmax(sorted_logits, dim=-1)\n",
    "\n",
    "        next_token = sorted_indices[torch.multinomial(filtered_probs, 1).item()].item()\n",
    "        generated.append(next_token)\n",
    "\n",
    "# ====== Decode Output ======\n",
    "result = ''.join([idx2char[i] for i in generated])\n",
    "print(\"\\n📝 Generated Text:\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ede0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
